"""ç»„åˆåˆ†æ API è·¯ç”±"""
import structlog
import numpy as np
from datetime import datetime
from fastapi import APIRouter, HTTPException, Query
from typing import List, Dict, Optional, Literal
from pydantic import BaseModel, Field

from services.data_router import MarketRouter

router = APIRouter(prefix="/portfolio", tags=["Portfolio"])
logger = structlog.get_logger()

PortfolioPeriod = Literal["1d", "5d", "1mo", "3mo", "6mo", "1y"]
DEFAULT_CLUSTER_THRESHOLD = 0.7
DEFAULT_MAX_SINGLE_WEIGHT = 0.45
DEFAULT_MAX_TOP2_WEIGHT = 0.65
DEFAULT_MAX_TURNOVER = 0.35


class RebalanceConstraints(BaseModel):
    """å†å¹³è¡¡çº¦æŸæ¡ä»¶ã€‚"""

    max_single_weight: float = Field(default=DEFAULT_MAX_SINGLE_WEIGHT, ge=0.1, le=0.9)
    max_top2_weight: float = Field(default=DEFAULT_MAX_TOP2_WEIGHT, ge=0.2, le=1.0)
    max_turnover: float = Field(default=DEFAULT_MAX_TURNOVER, ge=0.0, le=1.0)
    risk_profile: Literal["conservative", "balanced", "aggressive"] = "balanced"


class CorrelationRequest(BaseModel):
    """ç›¸å…³æ€§è®¡ç®—è¯·æ±‚"""

    symbols: List[str]
    period: PortfolioPeriod = "1mo"
    cluster_threshold: float = Field(
        default=DEFAULT_CLUSTER_THRESHOLD,
        ge=0.5,
        le=0.95,
        description="é£é™©èšç±»ç›¸å…³æ€§é˜ˆå€¼",
    )
    weights: Optional[List[float]] = Field(
        default=None,
        description="ç»„åˆæƒé‡ï¼ˆä¸ symbols é¡ºåºä¸€è‡´ï¼Œå¯ä¸ºä»»æ„éè´Ÿæ•°ï¼Œå°†è‡ªåŠ¨å½’ä¸€åŒ–ï¼‰",
    )
    constraints: Optional[RebalanceConstraints] = Field(default=None, description="å†å¹³è¡¡çº¦æŸ")
    enable_backtest_hint: bool = Field(default=True, description="æ˜¯å¦è¿”å›å›æµ‹è¯·æ±‚æç¤º")


class RiskCluster(BaseModel):
    """é£é™©èšç±»"""

    stocks: List[str]
    avg_correlation: float
    risk_level: str


class ConstraintViolation(BaseModel):
    """çº¦æŸè§¦å‘è¯´æ˜ã€‚"""

    code: str
    message: str
    actual: float
    limit: float
    severity: Literal["warning", "critical"] = "warning"


class RebalanceSuggestion(BaseModel):
    """å†å¹³è¡¡å»ºè®®æ¡ç›®"""

    symbol: str
    current_weight: float = Field(..., ge=0.0, le=1.0)
    target_weight: float = Field(..., ge=0.0, le=1.0)
    delta_weight: float
    action: Literal["increase", "decrease", "hold"]
    confidence: float = Field(..., ge=0.0, le=1.0)
    rationale: str
    volatility: float
    avg_abs_correlation: float
    total_return: float


class BacktestSignalHint(BaseModel):
    """å›æµ‹ä¿¡å·æç¤ºã€‚"""

    date: str
    signal: Literal["bullish", "bearish", "neutral"]
    confidence: float = Field(..., ge=0.0, le=1.0)
    source: str = "portfolio_rebalance"


class BacktestRequestHint(BaseModel):
    """å›æµ‹è¯·æ±‚æç¤ºã€‚"""

    symbol: str
    signals: List[BacktestSignalHint]
    initial_capital: float = 100000
    holding_days: int = 5
    stop_loss_pct: float = -5.0
    take_profit_pct: float = 10.0
    use_historical_signals: bool = False
    days_back: int = 180


class BacktestPayloadHint(BaseModel):
    """ç»„åˆåˆ†æç”Ÿæˆçš„å›æµ‹å…¥å‚å»ºè®®ã€‚"""

    strategy_name: str
    generated_at: str
    requests: List[BacktestRequestHint]


class CorrelationResult(BaseModel):
    """ç›¸å…³æ€§çŸ©é˜µç»“æœ"""

    symbols: List[str]
    matrix: List[List[float]]
    returns_summary: Dict[str, Dict[str, float]]


class PortfolioAnalysis(BaseModel):
    """ç»„åˆåˆ†æç»“æœ"""

    correlation: CorrelationResult
    diversification_score: float
    risk_clusters: List[RiskCluster]
    recommendations: List[str]
    rebalance_suggestions: List[RebalanceSuggestion] = Field(default_factory=list)
    recommended_turnover: float = Field(default=0.0, ge=0.0)
    constraint_violations: List[ConstraintViolation] = Field(default_factory=list)
    backtest_payload_hint: Optional[BacktestPayloadHint] = None


class QuickPortfolioCheckResponse(BaseModel):
    """å¿«é€Ÿç»„åˆæ£€æŸ¥å“åº”"""

    diversification_score: float
    risk_clusters_count: int
    top_recommendation: Optional[str] = None
    message: Optional[str] = None


def _normalize_weights(symbols: List[str], weights: Optional[List[float]]) -> Optional[List[float]]:
    """éªŒè¯å¹¶å½’ä¸€åŒ–ç»„åˆæƒé‡ã€‚"""
    if weights is None:
        return None

    if len(weights) != len(symbols):
        raise HTTPException(status_code=400, detail="weights é•¿åº¦å¿…é¡»ä¸ symbols ä¸€è‡´")

    if any(weight < 0 for weight in weights):
        raise HTTPException(status_code=400, detail="weights ä¸èƒ½ä¸ºè´Ÿæ•°")

    total_weight = float(sum(weights))
    if total_weight <= 0:
        raise HTTPException(status_code=400, detail="weights æ€»å’Œå¿…é¡»å¤§äº 0")

    return [float(weight) / total_weight for weight in weights]


def _parse_weights_query(weights: Optional[str]) -> Optional[List[float]]:
    """è§£æ quick-check query å‚æ•°ä¸­çš„æƒé‡ã€‚"""
    if weights is None or not weights.strip():
        return None

    parts = [part.strip() for part in weights.split(",") if part.strip()]
    if not parts:
        return None

    try:
        return [float(part) for part in parts]
    except ValueError as exc:
        raise HTTPException(status_code=400, detail="weights å¿…é¡»ä¸ºé€—å·åˆ†éš”çš„æ•°å­—") from exc


def _build_current_weights(
    correlation_symbols: List[str],
    request_symbols: List[str],
    normalized_request_weights: Optional[List[float]],
) -> np.ndarray:
    """æ„å»ºä¸æœ‰æ•ˆ symbols å¯¹é½çš„å½“å‰æƒé‡å‘é‡ã€‚"""
    n_symbols = len(correlation_symbols)
    if n_symbols == 0:
        return np.array([], dtype=float)

    if normalized_request_weights is None:
        return np.array([1.0 / n_symbols] * n_symbols, dtype=float)

    weight_map = {
        symbol: normalized_request_weights[index]
        for index, symbol in enumerate(request_symbols)
    }
    aligned_weights = np.array(
        [weight_map.get(symbol, 0.0) for symbol in correlation_symbols],
        dtype=float,
    )
    aligned_sum = float(np.sum(aligned_weights))
    if aligned_sum <= 0:
        return np.array([1.0 / n_symbols] * n_symbols, dtype=float)

    return aligned_weights / aligned_sum


def _normalize_probability_vector(weights: np.ndarray) -> np.ndarray:
    """å½’ä¸€åŒ–æƒé‡å‘é‡ï¼Œç¡®ä¿æ€»å’Œä¸º 1ã€‚"""
    clipped = np.clip(weights, 0.0, None)
    total = float(np.sum(clipped))
    if total <= 0:
        if len(clipped) == 0:
            return clipped
        return np.array([1.0 / len(clipped)] * len(clipped), dtype=float)
    return clipped / total


def _project_with_single_cap(weights: np.ndarray, max_single_weight: float) -> np.ndarray:
    """åœ¨ä¸Šé™çº¦æŸä¸‹æŠ•å½±æƒé‡ã€‚"""
    n = len(weights)
    if n == 0:
        return weights

    result = np.zeros(n, dtype=float)
    active = set(range(n))
    remaining = 1.0
    raw = np.clip(weights, 0.0, None)

    while active and remaining > 1e-12:
        raw_sum = float(np.sum([raw[index] for index in active]))
        distributed = False

        for index in list(active):
            if raw_sum > 0:
                proposed = remaining * (raw[index] / raw_sum)
            else:
                proposed = remaining / len(active)

            if proposed >= max_single_weight - 1e-12:
                result[index] = max_single_weight
                remaining -= max_single_weight
                active.remove(index)
                distributed = True

        if not distributed:
            for index in active:
                if raw_sum > 0:
                    result[index] = remaining * (raw[index] / raw_sum)
                else:
                    result[index] = remaining / len(active)
            remaining = 0.0

    if remaining > 1e-10:
        max_index = int(np.argmax(result)) if len(result) > 0 else 0
        result[max_index] += remaining

    return _normalize_probability_vector(result)


def _apply_rebalance_constraints(
    target_weights: np.ndarray,
    current_weights: np.ndarray,
    constraints: RebalanceConstraints,
) -> tuple[np.ndarray, List[ConstraintViolation]]:
    """åº”ç”¨å†å¹³è¡¡çº¦æŸå¹¶è¿”å›çº¦æŸè§¦å‘ä¿¡æ¯ã€‚"""
    n_symbols = len(target_weights)
    if n_symbols == 0:
        return target_weights, []

    violations: List[ConstraintViolation] = []

    effective_single_cap = constraints.max_single_weight
    if effective_single_cap * n_symbols < 1.0 - 1e-9:
        effective_single_cap = 1.0 / n_symbols
        violations.append(
            ConstraintViolation(
                code="single_cap_infeasible",
                message="å•ç¥¨ä¸Šé™è¿‡ä½ï¼Œå·²è‡ªåŠ¨æ”¾å®½åˆ°å¯è¡Œå€¼",
                actual=constraints.max_single_weight,
                limit=effective_single_cap,
                severity="critical",
            )
        )

    adjusted = _normalize_probability_vector(target_weights)

    before_single_max = float(np.max(adjusted))
    if before_single_max > effective_single_cap + 1e-9:
        violations.append(
            ConstraintViolation(
                code="single_cap_applied",
                message="å·²æŒ‰å•ç¥¨ä¸Šé™çº¦æŸè°ƒæ•´ç›®æ ‡æƒé‡",
                actual=before_single_max,
                limit=effective_single_cap,
                severity="warning",
            )
        )
    adjusted = _project_with_single_cap(adjusted, effective_single_cap)

    if n_symbols >= 2:
        top_indices = np.argsort(adjusted)[::-1][:2]
        top2_sum = float(adjusted[top_indices[0]] + adjusted[top_indices[1]])

        if n_symbols <= 2 and constraints.max_top2_weight < 1.0 - 1e-9:
            violations.append(
                ConstraintViolation(
                    code="top2_cap_infeasible",
                    message="å½“å‰æ ‡çš„æ•°é‡æ— æ³•æ»¡è¶³å‰ä¸¤å¤§ä»“ä½ä¸Šé™ï¼Œå·²å¿½ç•¥è¯¥çº¦æŸ",
                    actual=top2_sum,
                    limit=constraints.max_top2_weight,
                    severity="critical",
                )
            )
        elif top2_sum > constraints.max_top2_weight + 1e-9:
            violations.append(
                ConstraintViolation(
                    code="top2_cap_applied",
                    message="å·²æŒ‰å‰ä¸¤å¤§ä»“ä½ä¸Šé™è°ƒæ•´ç›®æ ‡æƒé‡",
                    actual=top2_sum,
                    limit=constraints.max_top2_weight,
                    severity="warning",
                )
            )

            excess = top2_sum - constraints.max_top2_weight
            if excess > 0 and n_symbols > 2:
                share_top_0 = adjusted[top_indices[0]] / top2_sum if top2_sum > 0 else 0.5
                reduce_top_0 = excess * share_top_0
                reduce_top_1 = excess - reduce_top_0
                adjusted[top_indices[0]] -= reduce_top_0
                adjusted[top_indices[1]] -= reduce_top_1

                other_indices = [index for index in range(n_symbols) if index not in top_indices]
                if other_indices:
                    capacities = np.array(
                        [max(0.0, effective_single_cap - adjusted[index]) for index in other_indices],
                        dtype=float,
                    )
                    capacity_sum = float(np.sum(capacities))
                    if capacity_sum > 0:
                        distribution = capacities / capacity_sum * excess
                        for offset, index in enumerate(other_indices):
                            adjusted[index] += float(distribution[offset])
                    else:
                        adjusted[top_indices[0]] += reduce_top_0
                        adjusted[top_indices[1]] += reduce_top_1

            adjusted = _project_with_single_cap(_normalize_probability_vector(adjusted), effective_single_cap)

    turnover = float(np.sum(np.abs(adjusted - current_weights)) / 2)
    if turnover > constraints.max_turnover + 1e-9:
        scale = constraints.max_turnover / turnover if turnover > 0 else 0.0
        adjusted = current_weights + (adjusted - current_weights) * scale
        adjusted = _project_with_single_cap(_normalize_probability_vector(adjusted), effective_single_cap)
        violations.append(
            ConstraintViolation(
                code="turnover_capped",
                message="å·²æŒ‰æœ€å¤§æ¢æ‰‹çº¦æŸæ”¶æ•›ç›®æ ‡æƒé‡",
                actual=turnover,
                limit=constraints.max_turnover,
                severity="warning",
            )
        )

    final_single = float(np.max(adjusted))
    if final_single > effective_single_cap + 1e-6:
        violations.append(
            ConstraintViolation(
                code="single_cap_unmet",
                message="åœ¨å½“å‰çº¦æŸç»„åˆä¸‹æ— æ³•å®Œå…¨æ»¡è¶³å•ç¥¨ä¸Šé™",
                actual=final_single,
                limit=effective_single_cap,
                severity="critical",
            )
        )

    if n_symbols >= 2:
        final_sorted = np.sort(adjusted)[::-1]
        final_top2 = float(final_sorted[0] + final_sorted[1])
        if final_top2 > constraints.max_top2_weight + 1e-6:
            violations.append(
                ConstraintViolation(
                    code="top2_cap_unmet",
                    message="åœ¨å½“å‰çº¦æŸç»„åˆä¸‹æ— æ³•å®Œå…¨æ»¡è¶³å‰ä¸¤å¤§ä»“ä½ä¸Šé™",
                    actual=final_top2,
                    limit=constraints.max_top2_weight,
                    severity="critical",
                )
            )

    final_turnover = float(np.sum(np.abs(adjusted - current_weights)) / 2)
    if final_turnover > constraints.max_turnover + 1e-6:
        violations.append(
            ConstraintViolation(
                code="turnover_unmet",
                message="åœ¨å½“å‰çº¦æŸç»„åˆä¸‹æ— æ³•å®Œå…¨æ»¡è¶³æœ€å¤§æ¢æ‰‹ä¸Šé™",
                actual=final_turnover,
                limit=constraints.max_turnover,
                severity="critical",
            )
        )

    return adjusted, violations


def _build_backtest_payload_hint(
    suggestions: List[RebalanceSuggestion],
    constraints: RebalanceConstraints,
) -> BacktestPayloadHint:
    """æ ¹æ®å†å¹³è¡¡å»ºè®®ç”Ÿæˆå›æµ‹å‚æ•°æç¤ºã€‚"""
    profile_to_holding_days = {
        "conservative": 10,
        "balanced": 7,
        "aggressive": 5,
    }

    requests: List[BacktestRequestHint] = []
    now_iso = datetime.utcnow().isoformat() + "Z"

    for suggestion in suggestions:
        signal: Literal["bullish", "bearish", "neutral"]
        if suggestion.action == "increase":
            signal = "bullish"
        elif suggestion.action == "decrease":
            signal = "bearish"
        else:
            signal = "neutral"

        confidence = float(np.clip(0.35 + abs(suggestion.delta_weight) * 4.0, 0.0, 1.0))

        requests.append(
            BacktestRequestHint(
                symbol=suggestion.symbol,
                signals=[
                    BacktestSignalHint(
                        date=now_iso,
                        signal=signal,
                        confidence=round(confidence, 3),
                    )
                ],
                holding_days=profile_to_holding_days[constraints.risk_profile],
                stop_loss_pct=-4.0 if constraints.risk_profile == "conservative" else -5.0 if constraints.risk_profile == "balanced" else -7.0,
                take_profit_pct=8.0 if constraints.risk_profile == "conservative" else 12.0 if constraints.risk_profile == "balanced" else 16.0,
                use_historical_signals=False,
                days_back=180,
            )
        )

    return BacktestPayloadHint(
        strategy_name=f"portfolio_rebalance_{constraints.risk_profile}",
        generated_at=now_iso,
        requests=requests,
    )


def _calculate_rebalance_suggestions(
    correlation: CorrelationResult,
    matrix: np.ndarray,
    current_weights: np.ndarray,
    constraints: RebalanceConstraints,
) -> tuple[List[RebalanceSuggestion], List[ConstraintViolation]]:
    """åŸºäºæ³¢åŠ¨ã€ç›¸å…³æ€§ä¸æ”¶ç›Šç”Ÿæˆå†å¹³è¡¡å»ºè®®ã€‚"""
    symbols = correlation.symbols
    n_symbols = len(symbols)

    if n_symbols == 0 or len(current_weights) != n_symbols:
        return [], []

    raw_scores: List[float] = []
    diagnostics: List[Dict[str, float]] = []

    for index, symbol in enumerate(symbols):
        metrics = correlation.returns_summary.get(symbol, {})
        volatility = max(float(metrics.get("volatility", 0.0)), 0.1)
        total_return = float(metrics.get("total_return", 0.0))

        if n_symbols > 1:
            row = matrix[index]
            avg_abs_corr = float(np.mean(np.abs(np.delete(row, index))))
        else:
            avg_abs_corr = 0.0

        if constraints.risk_profile == "conservative":
            momentum_score = float(np.clip((total_return + 18.0) / 45.0, 0.25, 1.15))
            stability_score = 1.0 / (volatility + 0.45)
            decorrelation_power = 1.0
        elif constraints.risk_profile == "aggressive":
            momentum_score = float(np.clip((total_return + 24.0) / 34.0, 0.25, 1.6))
            stability_score = 1.0 / (volatility + 0.85)
            decorrelation_power = 0.65
        else:
            momentum_score = float(np.clip((total_return + 20.0) / 40.0, 0.3, 1.4))
            stability_score = 1.0 / (volatility + 0.6)
            decorrelation_power = 0.8

        decorrelation_score = 1.0 - float(np.clip(avg_abs_corr, 0.0, 0.95))
        score = max(0.01, stability_score * (decorrelation_score ** decorrelation_power) * momentum_score)

        raw_scores.append(score)
        diagnostics.append(
            {
                "volatility": volatility,
                "avg_abs_corr": avg_abs_corr,
                "total_return": total_return,
            }
        )

    raw_scores_np = np.array(raw_scores, dtype=float)
    raw_sum = float(np.sum(raw_scores_np))
    if raw_sum <= 0:
        target_weights = np.array([1.0 / n_symbols] * n_symbols, dtype=float)
    else:
        target_weights = raw_scores_np / raw_sum

    adjusted_targets, constraint_violations = _apply_rebalance_constraints(
        target_weights=target_weights,
        current_weights=current_weights,
        constraints=constraints,
    )

    delta_weights = adjusted_targets - current_weights

    suggestions: List[RebalanceSuggestion] = []
    for index, symbol in enumerate(symbols):
        delta = float(delta_weights[index])
        if delta > 0.03:
            action: Literal["increase", "decrease", "hold"] = "increase"
        elif delta < -0.03:
            action = "decrease"
        else:
            action = "hold"

        metric = diagnostics[index]
        total_return = metric["total_return"]
        avg_abs_corr = metric["avg_abs_corr"]
        volatility = metric["volatility"]

        trend_desc = (
            "æ”¶ç›ŠåŠ¨èƒ½åå¼º"
            if total_return > 5
            else "è¿‘æœŸæ”¶ç›Šåå¼±"
            if total_return < -5
            else "æ”¶ç›Šè¡¨ç°ä¸­æ€§"
        )
        corr_desc = "ç›¸å…³æ€§è¾ƒä½" if avg_abs_corr < 0.35 else "ç›¸å…³æ€§åé«˜"
        vol_desc = "æ³¢åŠ¨è¾ƒä½" if volatility < 2 else "æ³¢åŠ¨ä¸­ç­‰" if volatility < 4 else "æ³¢åŠ¨åé«˜"

        if action == "increase":
            rationale = f"{trend_desc}ä¸”{corr_desc}ï¼Œ{vol_desc}ï¼Œå¯è€ƒè™‘é€‚åº¦å¢é…"
        elif action == "decrease":
            rationale = f"{corr_desc}æˆ–{vol_desc}ï¼Œå»ºè®®é€‚å½“é™é…å¹¶åˆ†æ•£é£é™©"
        else:
            rationale = f"{trend_desc}ã€{corr_desc}ï¼Œå½“å‰æƒé‡å¯ç»´æŒ"

        confidence = float(
            np.clip(0.45 + min(abs(delta) * 5.0, 1.0) * 0.35 + (0.15 if action != "hold" else 0.05), 0.0, 1.0)
        )

        suggestions.append(
            RebalanceSuggestion(
                symbol=symbol,
                current_weight=round(float(current_weights[index]), 6),
                target_weight=round(float(adjusted_targets[index]), 6),
                delta_weight=round(delta, 6),
                action=action,
                confidence=round(confidence, 3),
                rationale=rationale,
                volatility=round(volatility, 4),
                avg_abs_correlation=round(avg_abs_corr, 4),
                total_return=round(total_return, 4),
            )
        )

    suggestions.sort(key=lambda item: abs(item.delta_weight), reverse=True)
    return suggestions, constraint_violations


@router.post("/correlation", response_model=CorrelationResult)
async def calculate_correlation(request: CorrelationRequest):
    """
    è®¡ç®—è‚¡ç¥¨ç»„åˆçš„ç›¸å…³æ€§çŸ©é˜µ

    åŸºäºå†å²æ”¶ç›Šç‡è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°ã€‚
    """
    if len(request.symbols) < 2:
        raise HTTPException(status_code=400, detail="è‡³å°‘éœ€è¦ 2 ä¸ªè‚¡ç¥¨ä»£ç ")

    if len(request.symbols) > 20:
        raise HTTPException(status_code=400, detail="æœ€å¤šæ”¯æŒ 20 ä¸ªè‚¡ç¥¨")

    _normalize_weights(request.symbols, request.weights)

    try:
        returns_data: Dict[str, List[float]] = {}
        returns_summary: Dict[str, Dict[str, float]] = {}

        for symbol in request.symbols:
            try:
                history = await MarketRouter.get_history(symbol, request.period)
                if len(history) < 5:
                    logger.warning("Insufficient history data", symbol=symbol)
                    continue

                closes = [h.close for h in history]
                returns = []
                for i in range(1, len(closes)):
                    if closes[i - 1] != 0:
                        ret = (closes[i] - closes[i - 1]) / closes[i - 1]
                        returns.append(ret)

                if len(returns) > 0:
                    returns_data[symbol] = returns
                    returns_summary[symbol] = {
                        "mean_return": float(np.mean(returns) * 100),
                        "volatility": float(np.std(returns) * 100),
                        "total_return": float((closes[-1] / closes[0] - 1) * 100) if closes[0] != 0 else 0,
                        "data_points": len(returns)
                    }

            except Exception as e:
                logger.warning("Failed to get history for symbol", symbol=symbol, error=str(e))
                continue

        valid_symbols = list(returns_data.keys())
        if len(valid_symbols) < 2:
            raise HTTPException(
                status_code=400,
                detail=f"æ— æ³•è·å–è¶³å¤Ÿçš„å†å²æ•°æ®ã€‚æœ‰æ•ˆè‚¡ç¥¨: {valid_symbols}"
            )

        min_length = min(len(returns_data[s]) for s in valid_symbols)
        aligned_returns = {s: returns_data[s][-min_length:] for s in valid_symbols}

        n = len(valid_symbols)
        matrix = np.zeros((n, n))

        for i, s1 in enumerate(valid_symbols):
            for j, s2 in enumerate(valid_symbols):
                if i == j:
                    matrix[i][j] = 1.0
                else:
                    corr = np.corrcoef(aligned_returns[s1], aligned_returns[s2])[0, 1]
                    matrix[i][j] = float(corr) if not np.isnan(corr) else 0.0

        return CorrelationResult(
            symbols=valid_symbols,
            matrix=matrix.tolist(),
            returns_summary=returns_summary
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error("Correlation calculation failed", error=str(e))
        raise HTTPException(status_code=500, detail=f"è®¡ç®—å¤±è´¥: {str(e)}")


@router.post("/analyze", response_model=PortfolioAnalysis)
async def analyze_portfolio(request: CorrelationRequest):
    """
    å®Œæ•´çš„ç»„åˆåˆ†æ

    åŒ…æ‹¬ç›¸å…³æ€§çŸ©é˜µã€åˆ†æ•£åŒ–è¯„åˆ†ã€é£é™©èšç±»å’Œå»ºè®®ã€‚
    """
    normalized_request_weights = _normalize_weights(request.symbols, request.weights)
    constraints = request.constraints or RebalanceConstraints()

    if constraints.max_top2_weight + 1e-9 < constraints.max_single_weight:
        raise HTTPException(
            status_code=400,
            detail="constraints.max_top2_weight ä¸èƒ½å°äº constraints.max_single_weight",
        )

    correlation = await calculate_correlation(request)

    matrix = np.array(correlation.matrix)
    n = len(correlation.symbols)

    current_weights = _build_current_weights(
        correlation_symbols=correlation.symbols,
        request_symbols=request.symbols,
        normalized_request_weights=normalized_request_weights,
    )

    weights_used: Optional[np.ndarray] = current_weights if normalized_request_weights is not None else None

    if n > 1:
        upper_triangle = np.abs(matrix[np.triu_indices(n, k=1)])

        if weights_used is None:
            avg_corr = float(np.mean(upper_triangle))
            diversification_score = float((1 - avg_corr) * 100)
        else:
            pairwise_corrs = []
            pairwise_weights = []
            for i in range(n):
                for j in range(i + 1, n):
                    pairwise_corrs.append(abs(float(matrix[i][j])))
                    pairwise_weights.append(float(weights_used[i] * weights_used[j]))

            pairwise_corrs_np = np.array(pairwise_corrs, dtype=float)
            pairwise_weights_np = np.array(pairwise_weights, dtype=float)
            weight_sum = float(np.sum(pairwise_weights_np))

            if weight_sum > 0:
                avg_corr = float(np.dot(pairwise_corrs_np, pairwise_weights_np) / weight_sum)
            else:
                avg_corr = float(np.mean(upper_triangle))

            raw_diversification = float((1 - avg_corr) * 100)
            hhi = float(np.sum(np.square(weights_used)))
            concentration_ratio = max(0.0, (hhi - 1 / n) / (1 - 1 / n))
            concentration_penalty = concentration_ratio * 20.0
            diversification_score = max(0.0, min(100.0, raw_diversification - concentration_penalty))
    else:
        diversification_score = 0.0

    risk_clusters = []
    threshold = request.cluster_threshold

    visited = set()
    for i in range(n):
        if i in visited:
            continue

        cluster = [correlation.symbols[i]]
        visited.add(i)

        for j in range(i + 1, n):
            if j not in visited and matrix[i][j] >= threshold:
                cluster.append(correlation.symbols[j])
                visited.add(j)

        if len(cluster) > 1:
            avg_cluster_corr = np.mean([
                matrix[correlation.symbols.index(s1)][correlation.symbols.index(s2)]
                for s1 in cluster for s2 in cluster if s1 != s2
            ])
            risk_clusters.append({
                "stocks": cluster,
                "avg_correlation": float(avg_cluster_corr),
                "risk_level": "High" if avg_cluster_corr > 0.8 else "Moderate"
            })

    recommendations = []

    if diversification_score < 30:
        recommendations.append("âš ï¸ ç»„åˆé«˜åº¦é›†ä¸­ï¼Œå»ºè®®å¢åŠ ä¸åŒè¡Œä¸šæˆ–å¸‚åœºçš„è‚¡ç¥¨ä»¥é™ä½ç³»ç»Ÿæ€§é£é™©")
    elif diversification_score < 60:
        recommendations.append("ğŸ“Š ç»„åˆåˆ†æ•£åº¦ä¸­ç­‰ï¼Œå¯è€ƒè™‘å¢åŠ è´Ÿç›¸å…³æˆ–ä½ç›¸å…³èµ„äº§")
    else:
        recommendations.append("âœ… ç»„åˆåˆ†æ•£åº¦è‰¯å¥½ï¼Œé£é™©åˆ†å¸ƒè¾ƒä¸ºå‡è¡¡")

    if weights_used is not None:
        max_weight_idx = int(np.argmax(weights_used))
        max_weight_symbol = correlation.symbols[max_weight_idx]
        max_weight = float(weights_used[max_weight_idx])

        if max_weight >= 0.45:
            recommendations.append(
                f"âš–ï¸ æƒé‡é›†ä¸­åº¦åé«˜ï¼š{max_weight_symbol} æƒé‡çº¦ {max_weight * 100:.1f}%ï¼Œå»ºè®®é™ä½å•ç¥¨æš´éœ²"
            )

        top_two_weights = np.sort(weights_used)[-2:]
        top_two_share = float(np.sum(top_two_weights))
        if top_two_share >= 0.65:
            recommendations.append(
                f"ğŸ“‰ å‰ä¸¤å¤§æŒä»“å æ¯”çº¦ {top_two_share * 100:.1f}%ï¼Œå¯è€ƒè™‘å¼•å…¥ä½ç›¸å…³æ ‡çš„åˆ†æ•£é£é™©"
            )

    if risk_clusters:
        cluster_stocks = [", ".join(c["stocks"]) for c in risk_clusters]
        recommendations.append(
            f"ğŸ”— å‘ç°ç›¸å…³æ€§é«˜äºé˜ˆå€¼({threshold:.2f})çš„è‚¡ç¥¨ç¾¤: {'; '.join(cluster_stocks)}ã€‚è¿™äº›è‚¡ç¥¨å¯èƒ½åŒæ¶¨åŒè·Œ"
        )

    if correlation.returns_summary:
        high_vol = [s for s, data in correlation.returns_summary.items() if data.get("volatility", 0) > 3]
        if high_vol:
            recommendations.append(f"ğŸ“ˆ é«˜æ³¢åŠ¨è‚¡ç¥¨: {', '.join(high_vol)}ã€‚æ³¨æ„ä»“ä½æ§åˆ¶")

        negative_return = [s for s, data in correlation.returns_summary.items() if data.get("total_return", 0) < -10]
        if negative_return:
            recommendations.append(f"ğŸ“‰ è¿‘æœŸè¡¨ç°ä¸ä½³: {', '.join(negative_return)}ã€‚å»ºè®®å…³æ³¨åŸºæœ¬é¢å˜åŒ–")

    rebalance_suggestions, constraint_violations = _calculate_rebalance_suggestions(
        correlation=correlation,
        matrix=matrix,
        current_weights=current_weights,
        constraints=constraints,
    )
    recommended_turnover = float(
        np.sum([abs(item.delta_weight) for item in rebalance_suggestions]) / 2
    )

    actionable_suggestion = next(
        (item for item in rebalance_suggestions if item.action != "hold"),
        None,
    )
    if actionable_suggestion is not None:
        verb = "å¢æŒ" if actionable_suggestion.action == "increase" else "å‡æŒ"
        recommendations.append(
            f"ğŸ§­ å†å¹³è¡¡å»ºè®®ï¼š{actionable_suggestion.symbol} å¯è€ƒè™‘{verb}çº¦ {abs(actionable_suggestion.delta_weight) * 100:.1f}%"
        )

    if constraint_violations:
        recommendations.append(
            f"ğŸ›¡ï¸ çº¦æŸè§¦å‘ {len(constraint_violations)} é¡¹ï¼Œç›®æ ‡æƒé‡å·²æŒ‰ä¸Šé™è‡ªåŠ¨ä¿®æ­£"
        )

    backtest_payload_hint = (
        _build_backtest_payload_hint(rebalance_suggestions, constraints)
        if request.enable_backtest_hint and rebalance_suggestions
        else None
    )

    return PortfolioAnalysis(
        correlation=correlation,
        diversification_score=round(diversification_score, 1),
        risk_clusters=risk_clusters,
        recommendations=recommendations,
        rebalance_suggestions=rebalance_suggestions,
        recommended_turnover=round(recommended_turnover, 6),
        constraint_violations=constraint_violations,
        backtest_payload_hint=backtest_payload_hint,
    )


@router.get("/quick-check", response_model=QuickPortfolioCheckResponse)
async def quick_portfolio_check(
    symbols: str = Query(..., description="é€—å·åˆ†éš”çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨"),
    period: PortfolioPeriod = Query("1mo", description="å†å²æ•°æ®å‘¨æœŸ"),
    cluster_threshold: float = Query(
        DEFAULT_CLUSTER_THRESHOLD,
        ge=0.5,
        le=0.95,
        description="é£é™©èšç±»ç›¸å…³æ€§é˜ˆå€¼",
    ),
    weights: Optional[str] = Query(None, description="é€—å·åˆ†éš”çš„æƒé‡åˆ—è¡¨ï¼Œä¸ symbols ä¸€ä¸€å¯¹åº”"),
):
    """
    å¿«é€Ÿç»„åˆæ£€æŸ¥

    ç®€åŒ–ç‰ˆæ¥å£ï¼Œè¿”å›åŸºæœ¬çš„åˆ†æ•£åŒ–è¯„åˆ†ã€‚
    """
    symbol_list = [s.strip() for s in symbols.split(",") if s.strip()]

    if len(symbol_list) < 2:
        return QuickPortfolioCheckResponse(
            diversification_score=0.0,
            risk_clusters_count=0,
            message="è‡³å°‘éœ€è¦ 2 ä¸ªè‚¡ç¥¨",
        )

    try:
        parsed_weights = _parse_weights_query(weights)
        request = CorrelationRequest(
            symbols=symbol_list,
            period=period,
            cluster_threshold=cluster_threshold,
            weights=parsed_weights,
        )
        analysis = await analyze_portfolio(request)

        return QuickPortfolioCheckResponse(
            diversification_score=analysis.diversification_score,
            risk_clusters_count=len(analysis.risk_clusters),
            top_recommendation=analysis.recommendations[0] if analysis.recommendations else None,
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error("Quick portfolio check failed", symbols=symbol_list, error=str(e))
        raise HTTPException(status_code=500, detail=f"å¿«é€Ÿæ£€æŸ¥å¤±è´¥: {str(e)}")
