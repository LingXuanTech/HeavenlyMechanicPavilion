import asyncio
import re
import structlog
from fastapi import APIRouter, Depends, HTTPException
from sqlmodel import Session, select
from typing import List, Dict, Any, Optional
from db.models import ChatHistory, AnalysisResult, Watchlist, get_session
from config.settings import settings
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from services.prompt_manager import prompt_manager

router = APIRouter(prefix="/chat", tags=["Chat"])
logger = structlog.get_logger()


def _create_llm():
    """å»¶è¿Ÿåˆ›å»º LLM å®žä¾‹ï¼Œä¼˜é›…å¤„ç† API key ç¼ºå¤±"""
    if settings.GOOGLE_API_KEY:
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=settings.GOOGLE_API_KEY)
    elif settings.OPENAI_API_KEY:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model="gpt-4o-mini", api_key=settings.OPENAI_API_KEY)
    else:
        logger.warning("No LLM API key configured (OPENAI_API_KEY or GOOGLE_API_KEY)")
        return None


def _build_context(session: Session, message: str) -> str:
    """æž„å»º Fund Manager ä¸Šä¸‹æ–‡ä¿¡æ¯

    åŒ…å«ç”¨æˆ·è‡ªé€‰è‚¡åˆ—è¡¨å’Œæœ€è¿‘åˆ†æžç»“æžœï¼Œä»¥åŠ @symbol å¼•ç”¨çš„è‚¡ç¥¨æ•°æ®ã€‚

    Args:
        session: æ•°æ®åº“ä¼šè¯
        message: ç”¨æˆ·æ¶ˆæ¯ï¼ˆç”¨äºŽæå– @symbol å¼•ç”¨ï¼‰

    Returns:
        æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    """
    context_parts = []

    # 1. èŽ·å–ç”¨æˆ·è‡ªé€‰è‚¡åˆ—è¡¨
    try:
        watchlist = session.exec(select(Watchlist)).all()
        if watchlist:
            symbols = [w.symbol for w in watchlist]
            context_parts.append(f"ç”¨æˆ·è‡ªé€‰è‚¡: {', '.join(symbols)}")
    except Exception as e:
        logger.debug("Failed to load watchlist for chat context", error=str(e))

    # 2. æå– @symbol å¼•ç”¨å¹¶èŽ·å–å¯¹åº”åˆ†æžæ•°æ®
    mentioned_symbols = re.findall(r'@([A-Za-z0-9.]+)', message)

    # 3. èŽ·å–æœ€è¿‘çš„åˆ†æžç»“æžœï¼ˆè‡ªé€‰è‚¡ + @å¼•ç”¨çš„è‚¡ç¥¨ï¼‰
    target_symbols = set(mentioned_symbols)
    try:
        if watchlist:
            # æ·»åŠ è‡ªé€‰è‚¡ä¸­æœ€è¿‘åˆ†æžè¿‡çš„
            for w in watchlist[:5]:  # é™åˆ¶æ•°é‡é¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
                target_symbols.add(w.symbol)
    except Exception:
        pass

    for symbol in target_symbols:
        try:
            stmt = (
                select(AnalysisResult)
                .where(AnalysisResult.symbol == symbol, AnalysisResult.status == "completed")
                .order_by(AnalysisResult.created_at.desc())
                .limit(1)
            )
            result = session.exec(stmt).first()
            if result and result.result_json:
                import json
                data = json.loads(result.result_json) if isinstance(result.result_json, str) else result.result_json
                signal = data.get("signal", "N/A")
                confidence = data.get("confidence", "N/A")
                reasoning = data.get("reasoning", "")[:200]
                context_parts.append(
                    f"\nðŸ“Š {symbol} æœ€è¿‘åˆ†æž ({result.created_at.strftime('%Y-%m-%d %H:%M') if result.created_at else 'N/A'}):\n"
                    f"  ä¿¡å·: {signal} | ä¿¡å¿ƒåº¦: {confidence}\n"
                    f"  æ‘˜è¦: {reasoning}"
                )
        except Exception as e:
            logger.debug("Failed to load analysis for chat context", symbol=symbol, error=str(e))

    if not context_parts:
        return "æš‚æ— è‡ªé€‰è‚¡å’Œåˆ†æžæ•°æ®ã€‚"

    return "\n".join(context_parts)


def _get_system_prompt(context: str) -> str:
    """èŽ·å– Fund Manager ç³»ç»Ÿ prompt

    ä¼˜å…ˆä»Ž prompts.yaml åŠ è½½ï¼Œå¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤ promptã€‚

    Args:
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯å­—ç¬¦ä¸²

    Returns:
        å®Œæ•´çš„ç³»ç»Ÿ prompt
    """
    try:
        prompt_data = prompt_manager.get_prompt("fund_manager_chat", {"context": context, "message": ""})
        if prompt_data and prompt_data.get("system"):
            return prompt_data["system"]
    except Exception as e:
        logger.debug("Using default fund manager chat prompt", reason=str(e))

    # é»˜è®¤ promptï¼ˆprompts.yaml ä¸å¯ç”¨æ—¶çš„é™çº§ï¼‰
    return (
        "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„åŸºé‡‘ç»ç†ï¼ˆFund Managerï¼‰ï¼Œæ‹¥æœ‰ 15 å¹´ä»¥ä¸Šçš„æŠ•èµ„ç®¡ç†ç»éªŒã€‚\n"
        "ä½ ç®¡ç†ç€ä¸€åªå¤šç­–ç•¥åŸºé‡‘ï¼Œè¦†ç›– Aè‚¡ã€æ¸¯è‚¡å’Œç¾Žè‚¡å¸‚åœºã€‚\n\n"
        "è¯·åŸºäºŽä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å’ŒåŽ†å²å¯¹è¯ï¼Œæä¾›ä¸“ä¸šçš„æŠ•èµ„å»ºè®®ã€‚\n"
        "è§‚ç‚¹è¦æ˜Žç¡®ï¼Œç»™å‡ºå…·ä½“çš„æ“ä½œå»ºè®®ï¼Œå§‹ç»ˆé™„å¸¦é£Žé™©æç¤ºã€‚\n\n"
        f"## ä¸Šä¸‹æ–‡ä¿¡æ¯\n\n{context}\n\n"
        "è¾“å‡ºè¯­è¨€ï¼šç®€ä½“ä¸­æ–‡ã€‚"
    )


class ChatService:
    def __init__(self):
        self._llm = None

    @property
    def llm(self):
        """å»¶è¿Ÿåˆå§‹åŒ– LLM"""
        if self._llm is None:
            self._llm = _create_llm()
        return self._llm

    async def get_response(
        self,
        thread_id: str,
        message: str,
        history: List[ChatHistory],
        session: Optional[Session] = None,
    ) -> str:
        """ç”Ÿæˆ AI å›žå¤

        Args:
            thread_id: å¯¹è¯çº¿ç¨‹ ID
            message: ç”¨æˆ·æ¶ˆæ¯
            history: åŽ†å²å¯¹è¯è®°å½•
            session: æ•°æ®åº“ä¼šè¯ï¼ˆç”¨äºŽåŠ è½½ä¸Šä¸‹æ–‡ï¼‰

        Returns:
            AI å›žå¤å†…å®¹
        """
        if self.llm is None:
            raise HTTPException(status_code=503, detail="Chat service unavailable: No LLM API key configured")

        # æž„å»º Fund Manager ä¸Šä¸‹æ–‡
        context = ""
        if session:
            try:
                context = _build_context(session, message)
            except Exception as e:
                logger.warning("Failed to build chat context", error=str(e))
                context = "ä¸Šä¸‹æ–‡åŠ è½½å¤±è´¥ã€‚"

        system_prompt = _get_system_prompt(context)

        messages = [SystemMessage(content=system_prompt)]

        # é™åˆ¶åŽ†å²æ¶ˆæ¯æ•°é‡é¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
        recent_history = history[-20:] if len(history) > 20 else history
        for h in recent_history:
            if h.role == "user":
                messages.append(HumanMessage(content=h.content))
            else:
                messages.append(AIMessage(content=h.content))

        messages.append(HumanMessage(content=message))

        response = await self.llm.ainvoke(messages)
        return response.content

chat_service = ChatService()

from api.schemas.chat import ChatMessage, ChatResponse

@router.get("/{thread_id}", response_model=List[ChatMessage])
async def get_chat_history(thread_id: str, session: Session = Depends(get_session)):
    statement = select(ChatHistory).where(ChatHistory.thread_id == thread_id).order_by(ChatHistory.created_at)
    results = session.exec(statement).all()
    return [ChatMessage(role=r.role, content=r.content, timestamp=r.created_at.isoformat() if hasattr(r.created_at, 'isoformat') else str(r.created_at)) for r in results]

@router.post("/{thread_id}", response_model=ChatMessage)
async def send_message(thread_id: str, message: str, session: Session = Depends(get_session)):
    # Get history
    statement = select(ChatHistory).where(ChatHistory.thread_id == thread_id).order_by(ChatHistory.created_at)
    history = session.exec(statement).all()
    
    # Save user message
    user_msg = ChatHistory(thread_id=thread_id, role="user", content=message)
    session.add(user_msg)
    
    # Get AI response
    try:
        ai_content = await chat_service.get_response(thread_id, message, history, session=session)
        ai_msg = ChatHistory(thread_id=thread_id, role="assistant", content=ai_content)
        session.add(ai_msg)
        session.commit()
        
        return {"role": "assistant", "content": ai_content}
    except Exception as e:
        logger.error("Chat failed", error=str(e))
        raise HTTPException(status_code=500, detail="Chat service unavailable")
